<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Introduction to Computer Vision using OpenCV and Python | Virgilio</title>
    <meta name="generator" content="VuePress 1.8.0">
    
    <meta name="description" content="Data Science E-Learning">
    
    <link rel="preload" href="/Virgilio/assets/css/0.styles.b60dbc94.css" as="style"><link rel="preload" href="/Virgilio/assets/js/app.2e7e9464.js" as="script"><link rel="preload" href="/Virgilio/assets/js/2.f0423cde.js" as="script"><link rel="preload" href="/Virgilio/assets/js/15.971adb69.js" as="script"><link rel="prefetch" href="/Virgilio/assets/js/10.e5b0da50.js"><link rel="prefetch" href="/Virgilio/assets/js/11.e6289771.js"><link rel="prefetch" href="/Virgilio/assets/js/12.84859396.js"><link rel="prefetch" href="/Virgilio/assets/js/13.effd4111.js"><link rel="prefetch" href="/Virgilio/assets/js/14.afa7d231.js"><link rel="prefetch" href="/Virgilio/assets/js/16.f313e168.js"><link rel="prefetch" href="/Virgilio/assets/js/17.32bc2ea0.js"><link rel="prefetch" href="/Virgilio/assets/js/18.d970d3c9.js"><link rel="prefetch" href="/Virgilio/assets/js/19.0dd10b9b.js"><link rel="prefetch" href="/Virgilio/assets/js/20.692fb2ad.js"><link rel="prefetch" href="/Virgilio/assets/js/21.54ed5e12.js"><link rel="prefetch" href="/Virgilio/assets/js/22.31363364.js"><link rel="prefetch" href="/Virgilio/assets/js/23.0118caf3.js"><link rel="prefetch" href="/Virgilio/assets/js/24.badcc79f.js"><link rel="prefetch" href="/Virgilio/assets/js/25.ea6ad12e.js"><link rel="prefetch" href="/Virgilio/assets/js/26.e543a3e8.js"><link rel="prefetch" href="/Virgilio/assets/js/27.9e0e97ff.js"><link rel="prefetch" href="/Virgilio/assets/js/28.4438cf7d.js"><link rel="prefetch" href="/Virgilio/assets/js/29.bfdf0cf9.js"><link rel="prefetch" href="/Virgilio/assets/js/3.ee683496.js"><link rel="prefetch" href="/Virgilio/assets/js/30.33190243.js"><link rel="prefetch" href="/Virgilio/assets/js/31.60b4bf7e.js"><link rel="prefetch" href="/Virgilio/assets/js/32.453a1354.js"><link rel="prefetch" href="/Virgilio/assets/js/33.dfc4a6b8.js"><link rel="prefetch" href="/Virgilio/assets/js/34.bb45efc8.js"><link rel="prefetch" href="/Virgilio/assets/js/35.173b55bf.js"><link rel="prefetch" href="/Virgilio/assets/js/36.29bcaee3.js"><link rel="prefetch" href="/Virgilio/assets/js/37.2f01a325.js"><link rel="prefetch" href="/Virgilio/assets/js/38.b65cde1b.js"><link rel="prefetch" href="/Virgilio/assets/js/39.fb522d18.js"><link rel="prefetch" href="/Virgilio/assets/js/4.0eba860d.js"><link rel="prefetch" href="/Virgilio/assets/js/40.b2f1da71.js"><link rel="prefetch" href="/Virgilio/assets/js/41.e32f3bd0.js"><link rel="prefetch" href="/Virgilio/assets/js/42.e5c4d07f.js"><link rel="prefetch" href="/Virgilio/assets/js/43.3590abac.js"><link rel="prefetch" href="/Virgilio/assets/js/44.aaae622f.js"><link rel="prefetch" href="/Virgilio/assets/js/45.1d150652.js"><link rel="prefetch" href="/Virgilio/assets/js/46.b3cf7863.js"><link rel="prefetch" href="/Virgilio/assets/js/47.529d2feb.js"><link rel="prefetch" href="/Virgilio/assets/js/48.596010e7.js"><link rel="prefetch" href="/Virgilio/assets/js/49.3105c832.js"><link rel="prefetch" href="/Virgilio/assets/js/5.4e618a88.js"><link rel="prefetch" href="/Virgilio/assets/js/6.a077ea78.js"><link rel="prefetch" href="/Virgilio/assets/js/7.c38987bc.js"><link rel="prefetch" href="/Virgilio/assets/js/8.c4325bbd.js"><link rel="prefetch" href="/Virgilio/assets/js/9.64d913f9.js">
    <link rel="stylesheet" href="/Virgilio/assets/css/0.styles.b60dbc94.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/Virgilio/" class="home-link router-link-active"><!----> <span class="site-name">Virgilio <span class="site-name2"> Data Science </span></span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://github.com/virgili0/Virgilio" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Contribute
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://github.com/virgili0/Virgilio" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Contribute
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/Virgilio/" aria-current="page" class="sidebar-link">What is Virgilio?</a></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Paradiso</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/paradiso/demystification-ai-ml-dl.html" class="sidebar-link">Demystification of the key concepts of AI and ML</a></li><li><a href="/Virgilio/paradiso/what-do-i-need-for-ml.html" class="sidebar-link">What do I need to do Machine Learning?</a></li><li><a href="/Virgilio/paradiso/do-you-really-need-ml.html" class="sidebar-link">Do you really need Machine Learning?</a></li><li><a href="/Virgilio/paradiso/use-cases.html" class="sidebar-link">Machine Learning use cases</a></li><li><a href="/Virgilio/paradiso/virgilio-teaching-strategy.html" class="sidebar-link">Virgilio's Teaching Strategy</a></li><li><a href="/Virgilio/paradiso/introduction-to-ml.html" class="sidebar-link">Introduction to Machine Learning</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Purgatorio</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading open"><span>Fundamentals</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/fundamentals/math-fundamentals.html" class="sidebar-link">Mathematics</a></li><li><a href="/Virgilio/purgatorio/fundamentals/statistics-fundamentals.html" class="sidebar-link">Statistics</a></li><li><a href="/Virgilio/purgatorio/fundamentals/python-fundamentals.html" class="sidebar-link">Python</a></li><li><a href="/Virgilio/purgatorio/fundamentals/jupyter-notebook.html" class="sidebar-link">Jupyter Notebook</a></li><li><a href="/Virgilio/purgatorio/fundamentals/the-data-science-process.html" class="sidebar-link">The Data Science Process</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Define The Scope and Ask Questions</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/define-the-scope-and-ask-questions/frame-the-problem.html" class="sidebar-link">Frame the Problem</a></li><li><a href="/Virgilio/purgatorio/define-the-scope-and-ask-questions/usage-and-integration.html" class="sidebar-link">Usage and Integration</a></li><li><a href="/Virgilio/purgatorio/define-the-scope-and-ask-questions/starting-a-data-project.html" class="sidebar-link">Starting a Data Project</a></li><li><a href="/Virgilio/purgatorio/define-the-scope-and-ask-questions/workspace-setup-and-cloud-computing.html" class="sidebar-link">Workspace Setup and Cloud Computing</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Collect and Prepare Data</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/collect-and-prepare-data/data-collection.html" class="sidebar-link">Data Collection</a></li><li><a href="/Virgilio/purgatorio/collect-and-prepare-data/data-preparation.html" class="sidebar-link">Data Preparation</a></li><li><a href="/Virgilio/purgatorio/collect-and-prepare-data/data-visualization.html" class="sidebar-link">Data Visualization</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Select and Train Machine Learning Models</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/select-and-train-machine-learning-models/machine-learning-theory.html" class="sidebar-link">Machine Learning Theory</a></li><li><a href="/Virgilio/purgatorio/select-and-train-machine-learning-models/deep-learning-theory.html" class="sidebar-link">Deep Learning Theory</a></li><li><a href="/Virgilio/purgatorio/select-and-train-machine-learning-models/evaluation-and-finetuning.html" class="sidebar-link">Evaluation and Fine Tuning</a></li><li><a href="/Virgilio/purgatorio/select-and-train-machine-learning-models/tools-and-libraries.html" class="sidebar-link">Tools and Libraries</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Launch and Mantain the System </span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/launch-and-mantain-the-system/serving-trained-models.html" class="sidebar-link">Serving Trained Models</a></li><li><a href="/Virgilio/purgatorio/launch-and-mantain-the-system/monitoring-usage-and-behavior.html" class="sidebar-link">Monitoring Usage and Behavior</a></li><li><a href="/Virgilio/purgatorio/launch-and-mantain-the-system/automation-and-reproducibility.html" class="sidebar-link">Automation and Reproducibility</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Now Go Build </span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/now-go-build/a-messy-real-world.html" class="sidebar-link">A Messy Real World</a></li><li><a href="/Virgilio/purgatorio/now-go-build/transfer-learning.html" class="sidebar-link">Transfer Learning</a></li><li><a href="/Virgilio/purgatorio/now-go-build/best-practices.html" class="sidebar-link">Best Practices</a></li></ul></section></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Inferno</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Time Series</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/inferno/time-series/time-series.html" class="sidebar-link">Time Series Master Guide</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading open"><span>Computer Vision</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/inferno/computer-vision/introduction-to-computer-vision.html" aria-current="page" class="active sidebar-link">Introduction to Computer Vision using OpenCV and Python</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/Virgilio/inferno/computer-vision/introduction-to-computer-vision.html#prerequisites" class="sidebar-link">Prerequisites</a></li><li class="sidebar-sub-header"><a href="/Virgilio/inferno/computer-vision/introduction-to-computer-vision.html#guide-map" class="sidebar-link">Guide map</a></li><li class="sidebar-sub-header"><a href="/Virgilio/inferno/computer-vision/introduction-to-computer-vision.html#_1-introduction" class="sidebar-link">1.	Introduction:</a></li><li class="sidebar-sub-header"><a href="/Virgilio/inferno/computer-vision/introduction-to-computer-vision.html#_2-a-brief-introduction-to-deep-learning" class="sidebar-link">2.	A brief introduction to Deep Learning</a></li><li class="sidebar-sub-header"><a href="/Virgilio/inferno/computer-vision/introduction-to-computer-vision.html#_3-computer-vision-tasks" class="sidebar-link">3.	Computer vision tasks:</a></li><li class="sidebar-sub-header"><a href="/Virgilio/inferno/computer-vision/introduction-to-computer-vision.html#_4-computer-vision-systems" class="sidebar-link">4.	Computer Vision Systems</a></li><li class="sidebar-sub-header"><a href="/Virgilio/inferno/computer-vision/introduction-to-computer-vision.html#_5-python-libraries-for-computer-vision" class="sidebar-link">5.	Python libraries for Computer Vision</a></li><li class="sidebar-sub-header"><a href="/Virgilio/inferno/computer-vision/introduction-to-computer-vision.html#_6-opencv-library-on-windows-and-ubuntu" class="sidebar-link">6.		OpenCV library on Windows and Ubuntu</a></li><li class="sidebar-sub-header"><a href="/Virgilio/inferno/computer-vision/introduction-to-computer-vision.html#_7-processing-images-with-opencv" class="sidebar-link">7. Processing images with OpenCV</a></li><li class="sidebar-sub-header"><a href="/Virgilio/inferno/computer-vision/introduction-to-computer-vision.html#_8-use-cases-for-computer-vision" class="sidebar-link">8. Use cases for Computer Vision</a></li><li class="sidebar-sub-header"><a href="/Virgilio/inferno/computer-vision/introduction-to-computer-vision.html#_9-conclusions" class="sidebar-link">9. Conclusions</a></li></ul></li><li><a href="/Virgilio/inferno/computer-vision/object-instance-segmentation.html" class="sidebar-link">Object Instance Segmentation using TensorFlow Framework and Cloud GPU Technology</a></li><li><a href="/Virgilio/inferno/computer-vision/object-tracking.html" class="sidebar-link">Object Tracking based on Deep Learning</a></li><li><a href="/Virgilio/inferno/computer-vision/Object_detection_based_on_Deep_Learning.html" class="sidebar-link">Object detection based on Deep Learning</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Soft Skills</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/inferno/soft-skills/impactful-presentations.html" class="sidebar-link">Impactful Presentations</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Tools</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/inferno/tools/geo-gebra.html" class="sidebar-link">Geo Gebra</a></li><li><a href="/Virgilio/inferno/tools/latex.html" class="sidebar-link">LaTex</a></li><li><a href="/Virgilio/inferno/tools/regex.html" class="sidebar-link">Regex introduction</a></li><li><a href="/Virgilio/inferno/tools/wolfram-alpha.html" class="sidebar-link">Wolfram Alpha</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Research</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/inferno/research/zotero.html" class="sidebar-link">Zotero</a></li><li><a href="/Virgilio/inferno/research/sota-papers.html" class="sidebar-link">Research papers explained</a></li></ul></section></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="introduction-to-computer-vision-using-opencv-and-python"><a href="#introduction-to-computer-vision-using-opencv-and-python" class="header-anchor">#</a> Introduction to Computer Vision using OpenCV and Python</h1> <hr> <p><img src="https://i.ibb.co/L03Rvcq/introduction.jpg" alt="Introduction"></p> <p>In this guide, we will introduce a brief overview of Deep Learning. Then, we will discuss the purpose of Computer Vision in Python. After that, we' ll be taught the basics of dealing with data using OpenCV libraries by creating and displaying images. The fundamental tasks of Computer Vision such as object recognition and semantic segmentation will be explained. We will also cover the process of feature extraction, edge and face detection and object classification..</p> <h2 id="prerequisites"><a href="#prerequisites" class="header-anchor">#</a> Prerequisites</h2> <p>Before starting this guide, it is essential to be familiar with the basics of Python programming and Image Processing concepts.</p> <h2 id="guide-map"><a href="#guide-map" class="header-anchor">#</a> Guide map</h2> <p>We will provide a structured content according to the following map:</p> <ol><li>Introduction;</li> <li>A brief introduction to Deep Learning;</li> <li>Computer vision tasks;</li> <li>Computer Vision Systems;</li> <li>Python libraries for Computer Vision;</li> <li>OpenCV library on Windows and Ubuntu;</li> <li>Processing images with OpenCV;</li> <li>Use cases for Computer Vision;</li> <li>Conclusion.</li></ol> <h2 id="_1-introduction"><a href="#_1-introduction" class="header-anchor">#</a> 1.	Introduction:</h2> <p>Computer Vision is a branch of Computer Science, which aims to build up intelligent systems that can understand the content in images as they are perceived by humans. The data may be presented in different modalities such as sequential (video) images from multiple sensors (cameras) or multidimensional data from a biomedical camera, and so on. It is the discipline that integrates the methods of acquiring, processing, analyzing and understanding large-scale images from the real world. It is also about depicting and reconstructing the world that we perceive in images, such as edge, lighting, color and pattern. The recognition of images, by decoding them into meaningful information from image-based data using models created by engineering, physics, statistics and learning theories. It is intended to simulate human vision, including the ability to learn, make decisions and react to actions based on visual information. Computer Vision is one aspect of Artificial Intelligence and Image Processing, which generally aims to simulate intelligent human capabilities. In computer Vision concept, object recognition is one of the fundamental tasks, which depends on how these objects are defined, whether in the form of images or video sequences, and human beings are able to recognize many entities, even if these objects, which are images, vary greatly in size and lighting.</p> <p><img src="https://i.ibb.co/X5d74Ft/CVV.jpg" alt="Computer Vision"></p> <p>Some examples of Computer Vision applications:</p> <ul><li>Any application that can recognize objects or humans in an image;</li> <li>Automatic control applications (industrial robots, vehicles);</li> <li>Object construction models (industrial inspection, medical image analysis);</li> <li>Applications make it possible to track a moving object.</li></ul> <p>Useful books for learning various aspects of Computer Vision: <a href="http://www.robots.ox.ac.uk/~vgg/hzbook/" target="_blank" rel="noopener noreferrer">Multiple View Geometry in Computer Vision<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>,  <a href="http://szeliski.org/Book/" target="_blank" rel="noopener noreferrer">Computer Vision: Algorithms and Applications<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <h2 id="_2-a-brief-introduction-to-deep-learning"><a href="#_2-a-brief-introduction-to-deep-learning" class="header-anchor">#</a> 2.	A brief introduction to Deep Learning</h2> <h3 id="_2-1-what-is-deep-learning"><a href="#_2-1-what-is-deep-learning" class="header-anchor">#</a> 2.1. What is Deep Learning?</h3> <p>Deep Learning is an Machine Learning strategy that has greatly enhanced performance in many fields such as Computer Vision, Speech Recognition, Machine Tanslation, and so on.  The use of deep learning techniques, through raw data, allows many challenges to be solved in many economic sectors such as health, transport, finance, etc.</p> <p>The favourable conditions that allowed the rise of Deep Learning:</p> <ul><li>Availability of very large spatio-temporal datasets (Big Data);</li> <li>Availability of high-performance computing (GPU);</li> <li>Flexibility of new training models (Deep Neural Networks).</li></ul> <h3 id="_2-2-deep-learning-frameworks"><a href="#_2-2-deep-learning-frameworks" class="header-anchor">#</a> 2.2. Deep Learning Frameworks</h3> <p>In this section, we present the most popular frameworks for Deep Learning.</p> <table><thead><tr><th>Framework</th> <th>Features</th> <th>Supports languages</th> <th>Download</th></tr></thead> <tbody><tr><td>Tensorflow</td> <td>Highly flexible system architecture</td> <td>Python, C++ and R</td> <td><a href="https://www.tensorflow.org/" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></td></tr> <tr><td>Caffe</td> <td>Speed, transposability and applicability in modelling Convolution Neural Networks (CNN)</td> <td>C, C++, Python, MATLAB</td> <td><a href="http://caffe.berkeleyvision.org/" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></td></tr> <tr><td>CNTK</td> <td>Easy training and combination of popular model types across servers</td> <td>Python, C++ and the Command Line Interface</td> <td><a href="https://www.microsoft.com/en-us/cognitive-toolkit/" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></td></tr> <tr><td>Torch/PyTorch</td> <td>The entire deep modeling process is far more simpler as well as transparent</td> <td>Lua, Python</td> <td><a href="http://torch.ch/" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> or <a href="http://pytorch.org/" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></td></tr> <tr><td>Keras</td> <td>Provide a simplistic interface for the purpose of quick prototyping by constructing effective neural networks that can work with TensorFlow</td> <td>Python</td> <td><a href="https://keras.io/" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></td></tr></tbody></table> <h2 id="_3-computer-vision-tasks"><a href="#_3-computer-vision-tasks" class="header-anchor">#</a> 3.	Computer vision tasks:</h2> <p>In this section, we will successively examine some tasks of Computer Vision, in particular Image Recognition, Semantic Segmentation, Image Retrieval, Image Restoration, Object Recognition, Video Tracking, and so on.</p> <p><img src="https://i.ibb.co/NxVsrHf/cvt.jpg" alt="CV tasks"></p> <h3 id="_3-1-image-recognition"><a href="#_3-1-image-recognition" class="header-anchor">#</a> 3.1.	 Image Recognition</h3> <p>Traditionally, Computer Vision is about deciding whether or not the image contains an object. This task can be solved simply with little effort by human beings, but a certain activity is still not solved effectively and finely by computer in its general state. The only way to solve this issue is to find the best solutions to match certain features (edges, shapes, etc), and in some cases only, often with specific lighting conditions, a background and a certain position for the camera.</p> <h4 id="types-of-recognition"><a href="#types-of-recognition" class="header-anchor">#</a> Types of recognition:</h4> <p><strong>A - Identification:</strong>
Predefined objects are often identified from different viewpoints of the camera in their different locations.</p> <p><strong>B - Selection:</strong>
Define a unique identifier in the shape. For example: identify a person's face or identify the specific type of a person or car.</p> <p><strong>C - Examination:</strong>
Image data is treated for a specific object. For example: check for the presence of diseased cells in medical form, check if a car is present on a highway.</p> <p>You will find here some projects and scripts based on Image Recognition task using a Deep Learning paradigm:</p> <ul><li>Fast MPN-COV: <a href="https://github.com/jiangtaoxie/fast-MPN-COV" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li>Fine-Grained Representation Learning and Recognition by Exploiting Hierarchical Semantic Embedding: <a href="https://github.com/HCPLab-SYSU/HSE" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li>Fine grained classification: <a href="https://github.com/xcnkx/fine_grained_classification" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</li></ul> <h3 id="_3-2-image-retrieval"><a href="#_3-2-image-retrieval" class="header-anchor">#</a> 3.2.	Image Retrieval:</h3> <p>Images stored in a visual dataset are retrieved based on the content as well as similar concepts of the database query where an image is inserted, and the output is a similar set of images. Content-based visual information retrieval is the implementation of the computer vision system in order to target images, i.e. the problem of retrieving images from large datasets. Image retrieval systems seek to find images similar to a query image among a dataset. The following figure represents the general process of retrieving images from content.</p> <p><img src="https://i.ibb.co/VTLpZgD/SFS.jpg" alt="Image retrieval"></p> <p>You will find here some projects and scripts based on Image Retrieval tasks using a Deep Learning paradigm:</p> <ul><li>Deep Local Feature (DeLF): <a href="https://github.com/nashory/DeLF-pytorch" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li>MILDNet: <a href="https://github.com/gofynd/mildnet" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li>MultiGrain: <a href="https://github.com/facebookresearch/multigrain" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</li></ul> <h3 id="_3-3-image-restoration"><a href="#_3-3-image-restoration" class="header-anchor">#</a> 3.3.	Image Restoration:</h3> <p>This is the process of restoring degraded images that cannot be recovered. Original images can be restored by prior-knowledge of damage or distortions that cause deterioration of images such as scratches, dust and stains. Restoration also includes images taken by sophisticated cameras that have been distorted due to the weather conditions in which they were taken, such as scanned images.</p> <p>You will find here some projects and scripts based on Image Restoration task using a Deep Learning paradigm:</p> <ul><li>Image Super Resolution using in Keras 2+: <a href="https://github.com/titu1994/Image-Super-Resolution" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li>RED-net: <a href="https://github.com/ved27/RED-net" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li>Noise2Noise: <a href="https://github.com/NVlabs/noise2noise" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</li></ul> <h3 id="_3-4-object-recognition"><a href="#_3-4-object-recognition" class="header-anchor">#</a> 3.4.	Object Recognition:</h3> <p>It is a branch of Computer Vision dedicated to the detection of a particular object in an image or video. Humans can recognize many objects in images with little effort, although the image may differ slightly from different aspects, such as variations, or even when they are moved or rotated. Although humans can recognize objects when they are partially hidden, this task remains a challenge for computer vision systems. The  object recognition process is given by the following figure:</p> <p><img src="https://www.mathworks.com/content/mathworks/www/en/solutions/deep-learning/object-recognition/jcr:content/mainParsys/band_copy_1227855798_2052140687/mainParsys/columns_1606542234/2/image_copy_copy_copy.adapt.full.low.svg/1541451136966.svg" alt="Object recognition"></p> <p>Unlike Machine Learning, the Deep Learning paradigm consists of an end-to-end feature representation learning from raw data without any prior data processing steps.</p> <p>You will find here some projects and scripts based on Object Recognition task using a Deep Learning paradigm:</p> <ul><li>TF-slim: <a href="https://github.com/tensorflow/models/tree/master/research/slim" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li>DenseNet: <a href="https://github.com/liuzhuang13/DenseNet" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li>DeepBeliefSDK: <a href="https://github.com/jetpacapp/DeepBeliefSDK" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</li></ul> <h3 id="_3-5-semantic-segmentation"><a href="#_3-5-semantic-segmentation" class="header-anchor">#</a> 3.5. Semantic Segmentation</h3> <p>Semantic segmentation is a Deep Learning algorithm that assigns a label or category for each pixel in an image. It makes it possible to recognize a set of pixels that are in distinct classes. For example, an autonomous vehicle must be able to recognize vehicles, pedestrians, traffic signs, sidewalks and other environmental components of the road network. Semantic segmentation is involved in a wide range of solutions such as computer-controlled driving, autonomous vehicles, diagnostic imaging, industrial controls, and so on. The splitting of images into two classes is a simple example of semantic segmentation. In fact, it has no restriction in terms of categories. The number of classes can be changed in order to classify image content. For example, the image could be segmented into 4 classes: person, sky, sea and background. The example in the following figure is based on ICCV 2015 paper <a href="http://www.robots.ox.ac.uk/~szheng/papers/CRFasRNN.pdf" target="_blank" rel="noopener noreferrer">Conditional Random Fields as Recurrent Neural Networks<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, which utilizes deep learning techniques and probabilistic graphical models for semantic image segmentation.</p> <p><img src="https://i.ibb.co/gdWgGd4/ss.jpg" alt="Semantic Segmentation"></p> <p>You will find here some projects and scripts based on Semantic Segmentation task using a Deep Learning paradigm:</p> <ul><li>PSPNet: <a href="https://github.com/hszhao/PSPNet" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li>TorchSeg: <a href="https://github.com/ycszen/TorchSeg" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li>Deeplab: <a href="https://github.com/tensorflow/models/tree/master/research/deeplab" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</li></ul> <h3 id="_3-6-video-tracking"><a href="#_3-6-video-tracking" class="header-anchor">#</a> 3.6.	Video Tracking</h3> <p>It is the process of locating or tracking a moving object (or several moving objects) using static or mobile cameras, while having many uses, such as human-computer interaction, security, three-dimensional reality, medical images and video editing. Tracking can be time-consuming due to video content and the need to use complex algorithms to identify and track objects.
Tracking aims to follow the desired object to be tracked in a sequence of successive images. Tracking is a difficult task when this object moves faster than the capture-rate of these successive images. It is even more difficult when this entity changes direction as it shifts. For this reason, tracking systems apply a motion model that explains how this object's image will change as it moves in different directions. The following figure illustrates the overall scheme of the object tracking process:</p> <p><img src="https://i.ibb.co/hY5GhdB/track.jpg" alt="Object Tracking"></p> <p>You will find here some projects and scripts based on Video Tracking task using a Deep Learning paradigm:</p> <ul><li>GOT-10k Python Toolkit: <a href="https://github.com/got-10k/toolkit" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li>SiamMask: <a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li>Deep SORT: <a href="https://github.com/guanfuchen/deep_sort" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li>Object tracking (tutorial): <a href="https://www.pyimagesearch.com/2018/07/30/opencv-object-tracking/" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</li></ul> <h2 id="_4-computer-vision-systems"><a href="#_4-computer-vision-systems" class="header-anchor">#</a> 4.	Computer Vision Systems</h2> <p>Computer vision systems are very diverse and are divided into large and sophisticated systems that perform general and complete tasks as well as small systems that perform specific and simple ones. Most computer vision systems mainly include the following:</p> <h3 id="_4-1-collecting-images"><a href="#_4-1-collecting-images" class="header-anchor">#</a> 4.1.	Collecting images</h3> <p>The image is generated by using one or more image sensors. These include many digital camera sensors, distance sensors, radars, and ultrasonic cameras.</p> <h3 id="_4-2-pre-processing-operations"><a href="#_4-2-pre-processing-operations" class="header-anchor">#</a> 4.2.	Pre-processing operations</h3> <p>Before applying the computer vision algorithm in order to extract valuable information, it is necessary to perform prior data operations to ensure that the data are consistent with the algorithm's specific hypotheses. Some examples of these processes include:</p> <ol><li>Select the image resolution to confirm that its coordinate system is correct.</li> <li>Reduce the interference to ensure that the sensor does not provide inaccurate information.</li> <li>Increase the variance in order to ensure that the required information will be available.</li></ol> <h3 id="_4-3-features-extraction"><a href="#_4-3-features-extraction" class="header-anchor">#</a> 4.3.	Features extraction</h3> <p>Visual data features are extracted at different levels of abstraction from data raw. These benchmarks are categorized into:</p> <ol><li>Global features such as color and shape.</li> <li>Local features such as edges and points.
More complex features related to colors and patterns can be obtained.</li></ol> <h3 id="_4-4-segmentation"><a href="#_4-4-segmentation" class="header-anchor">#</a> 4.4.	Segmentation</h3> <p>All zones of the image can be recognized as important locations for subsequent operations. For example: select a set of key points, divide one or more images that contain the region of interest.
3.5.	High-level processing operations
At this stage, the input data consists of a small set of data, such as a set of points or a portion of the image that is suspected to contain the interest object. The other operations are:</p> <ol><li>Ensure that the collected data are consistent with the hypotheses of the intended application.</li> <li>Evaluate the transaction values assigned to the request, such as steering or shape size.</li> <li>Classify the recognized objects into several classes</li></ol> <h2 id="_5-python-libraries-for-computer-vision"><a href="#_5-python-libraries-for-computer-vision" class="header-anchor">#</a> 5.	Python libraries for Computer Vision</h2> <p>The main toolkits for image processing in python are OpenCV, scikit-image and Pillow. The most general Python libraries (Numpy and Scipy) also provide some image processing tools. All these libraries can easily dialog with each other due to the common use of Numpy arrays to store images. A grayscale image is usually stored in a 2-dimensional integer or real value Numpy array with H rows and W columns (W=width,H=height). A color image is stored in a 3-dimensional Numpy array (H, W, 3).</p> <ul><li>OpenCV is a library that is written in C++, which is rich and widely used in computer vision.</li> <li><a href="http://pillow.readthedocs.org/en/latest/" target="_blank" rel="noopener noreferrer">Pillow<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> is a PIL Fork (Python IMage Library). It is a library that is specific to Python, but is mainly written in C. It allows basic operations to be performed on images including read/write, transformations, histograms, filtering.</li> <li><a href="http://scikit-image.org/" target="_blank" rel="noopener noreferrer">Scikit-Imag<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>e is a fairly recent and actively developed library. The advantage of this library is that it is written in Python and Cython (Python typed and compiled for acceleration) which makes it easy to read its code.</li> <li>[Scipy.ndimage](http://docs.scipy.org/doc/scipy/reference/ndimage.
htm): Scipy's ndimage module provides a number of functions for shaping, interpolation, mathematical morphology and statistics.</li></ul> <h2 id="_6-opencv-library-on-windows-and-ubuntu"><a href="#_6-opencv-library-on-windows-and-ubuntu" class="header-anchor">#</a> 6.		OpenCV library on Windows and Ubuntu</h2> <p>Gary Bradsky started OpenCV at Intel in 1999. Compatible with a variety of languages such as C++, Python, etc., OpenCV-Python is an API that allows OpenCV to simultaneously release the power of Python and C++ API. In the case of Python, it is a library of binaries intended to address computer vision challenges. This library is based on NumPy and its array structures. That means we can also integrate it easily into other libraries such as SciPy and Matplotlib.</p> <p>As we have explained previously, all operations on images are purely mathematical operations. But we can't say that programmers will do all these operations every time they use images, hence the development of OpenCV library, which includes functions that perform the most necessary operations in the images.</p> <h3 id="windows"><a href="#windows" class="header-anchor">#</a> Windows:</h3> <p>In order to download the Python program (x,y), click [here](https://python-xy.github.io/downloads.html, it’s possible to download each file individually).  First, download the following <a href="https://drive.google.com/file/d/0B0kFf-FN5r9tS1EzSlppUGItNUU/view?usp=sharing" target="_blank" rel="noopener noreferrer">file<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> which contains the collection of the OpenCV library. Then, install the python program (x,y) as shown in the figures:
<img src="https://i.ibb.co/WgLMMHs/122.png" alt="Windows"></p> <h3 id="ubuntu"><a href="#ubuntu" class="header-anchor">#</a> Ubuntu:</h3> <p>There are two ways to install OpenCV on Linux systems. The first one consists in installing pre-compiled files from repositories. For instance, in the case of the Ubuntu platform, it is sufficient to execute the following command:</p> <div class="language- extra-class"><pre class="language-text"><code>sudo apt-get install libopencv-dev python-opencv
</code></pre></div><p>The second method consists in compiling the source files immediately beforehand (this method allows you to obtain the latest version of the library).</p> <p>Open the terminal line and proceed as follows:</p> <div class="language- extra-class"><pre class="language-text"><code>sudo apt-get update 
sudo apt-get upgrade
sudo apt-get install build-essential cmake git pkg-config
sudo apt-get install libjpeg8-dev libtiff4-dev libjasper-dev libpng12-dev 
sudo apt-get install libatlas-base-dev gfortran
# install Pip package 
wget https://bootstrap.pypa.io/get-pip.py 
sudo python get-pip.py
sudo pip install virtualenv virtualenvwrapper 
sudo rm -rf ~/.cache/pip
# virtualenv and virtualenvwrapper 
export WORKON_HOME=$HOME/.virtualenvs source /usr/local/bin/virtualenvwrapper.sh
source ~/.bashrc
mkvirtualenv cv
# Install Python2.7 
sudo apt-get install python2.7-dev
# Install Numby libraries
pip install numpy
# Download OpenCV library

cd ~ 
git clone https://github.com/Itseez/opencv.git 
cd opencv 
git checkout 3.0.0

</code></pre></div><p>After that, try downloading the opencv_contrib package. It will be used to use some features such as SIFT, SURF, which were in the OpenCV 2.4.2 library, and then deleted in OpenCV 3.0.</p> <div class="language- extra-class"><pre class="language-text"><code>cd ~ 
git clone https://github.com/Itseez/opencv_contrib.git 
cd opencv_contrib
git checkout 3.0.0
cd ~/opencv 
mkdir build 
cd build
cmake -D CMAKE_BUILD_TYPE=RELEASE \ -D CMAKE_INSTALL_PREFIX=/usr/local \ -D INSTALL_C_EXAMPLES=ON \ -D INSTALL_PYTHON_EXAMPLES=ON \ -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib/modules \ -D BUILD_EXAMPLES=ON ..
make
sudo make install
sudo ldconfig
cd ~/.virtualenvs/cv/lib/python2.7/site-packages/ 
ln -s /usr/local/lib/python2.7/site-packages/cv2.so cv2.so


</code></pre></div><h2 id="_7-processing-images-with-opencv"><a href="#_7-processing-images-with-opencv" class="header-anchor">#</a> 7. Processing images with OpenCV</h2> <p>Now we have successfully installed OpenCV, let's start by doing it.</p> <h3 id="_7-1-reading-images-in-python"><a href="#_7-1-reading-images-in-python" class="header-anchor">#</a> 7.1. Reading images in Python</h3> <p>To read an image, we have the <code>imread ()</code> function. It should be mentioned that previously, we have moved to the directory that contains the image.</p> <div class="language- extra-class"><pre class="language-text"><code>img = cv2.imread ('img.jpg')

</code></pre></div><p>As an alternative, it is also possible to pass a value for a flag, which is the second argument</p> <p>cv2.IMREAD_COLOR: For loading a color image by overlooking existing transparency;
cv2.IMREAD_GRAYSCALE: For loading a grayscale image;
cv2.IMREAD_UNCHANGED: For loading an image that includes an alpha channel
It is possible to use integers 1, 0 or -1:</p> <div class="language- extra-class"><pre class="language-text"><code>img = cv2. imread ('img.jpg', 0)

</code></pre></div><p>Note that sending an invalid image path does not result in any errors.</p> <h3 id="_7-2-displaying-images-in-python"><a href="#_7-2-displaying-images-in-python" class="header-anchor">#</a> 7.2. Displaying images in Python</h3> <p>The <code>cv2.imshow ()</code> function enables to display an image in a frame that can be adjusted to its size. The first argument is the name of the frame and the second one is the image.</p> <div class="language- extra-class"><pre class="language-text"><code>
img = cv2. imread ('img.jpg')
cv2.imshow('Images', img)
</code></pre></div><p>Note that we have two frames at once as we have not attempted to title them in the same way. <code>cv2.destroyAllWindows ()</code> function is another function that destroys all the frames that we have already created. <code>cv2.destroyWindow ()</code> also destroys a specific frame.</p> <h3 id="_7-3-creating-images-in-python"><a href="#_7-3-creating-images-in-python" class="header-anchor">#</a> 7.3. Creating images in Python</h3> <p>To do this, there is the <code>cv2.imwrite ()</code>function. The first argument is the file name and the second one is the image to be saved.</p> <div class="language- extra-class"><pre class="language-text"><code>cv2.imwrite('img_gray.png', img)

</code></pre></div><p>This will store the grayscale image named &quot;img_gray.png&quot; in the current location.</p> <h3 id="_7-4-displaying-images-using-matplotlib"><a href="#_7-4-displaying-images-using-matplotlib" class="header-anchor">#</a> 7.4. Displaying images using Matplotlib</h3> <p>By using <a href="https://matplotlib.org/index.html" target="_blank" rel="noopener noreferrer">Matplotlib<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> library, we can display that image.</p> <div class="language- extra-class"><pre class="language-text"><code>import matplotlib.pyplot as plt
plt.imshow(img, cmap = &quot;gray&quot;, interpolation = &quot;bilinear&quot;)
plt.xticks([]), pl.ticks ([])
(([], ), ([], ))
plt.display ()
</code></pre></div><h3 id="_7-5-core-operations-on-images"><a href="#_7-5-core-operations-on-images" class="header-anchor">#</a> 7.5. Core operations on images</h3> <p>Let's now look at the basic operations applicable on the image.</p> <div class="language- extra-class"><pre class="language-text"><code>import cv2
img = cv2.imread ('img.jpg')
y, x = 100,50
</code></pre></div><p>Reading of color values at positions y, x:</p> <div class="language- extra-class"><pre class="language-text"><code>(b, g, r) = img[y,x]

</code></pre></div><p>Region of interest at (x, y) whose dimensions are 100x100:</p> <div class="language- extra-class"><pre class="language-text"><code>roi = img[y:y+100,x:x+100] 
cv2.imshow ('image', img)
cv2.imshow('ROI', roi)
</code></pre></div><p>Pixelization of the new color :</p> <div class="language- extra-class"><pre class="language-text"><code>
roi[:,:]= (55,44,87) 
cv2.imshow('New image', img)
</code></pre></div><h2 id="_8-use-cases-for-computer-vision"><a href="#_8-use-cases-for-computer-vision" class="header-anchor">#</a> 8. Use cases for Computer Vision</h2> <p>In this section, we will look at some tasks related to computer vision such as *edge detection, face detection, *feature detection and description, object classification performed by OpenCV and Python.</p> <h3 id="_8-1-edge-detection"><a href="#_8-1-edge-detection" class="header-anchor">#</a> 8.1.Edge detection</h3> <p>In OpenCV we can choose only to display the edges of objects with the <code>Canny ()</code> function:</p> <div class="language- extra-class"><pre class="language-text"><code>import numpy as np
img = cv2.imread('img.jpg')
cv2.imwrite ('edge_img.jpg', cv2.Canny (img, 512, 415))
cv2.imshow ('edges', cv2.imread('edge_img.jpg'))
</code></pre></div><h3 id="_8-2-face-detection"><a href="#_8-2-face-detection" class="header-anchor">#</a> 8.2. Face detection</h3> <p>OpenCV will also enable to detect faces in images. Let's now use Haar's cascading classifier.</p> <p>Now, there is one last point that we would really like to address, and that is the face detection. The Haar classifier is used. It is a matter of locating the position of faces in an image in order to standardize the size of the face area.</p> <div class="language- extra-class"><pre class="language-text"><code>import sys, os
import cv2
  
def face_detection(image, image_out, show = False):
    # Load the image in memory
    img = cv2.imread(image)
    # Load the face detection model
    face_model = cv2.CascadeClassifier(&quot;haarcascade_frontalface_alt2.xml&quot;)
     
     
    # detection of the face(s)
    faces = face_model.detectMultiScale(img)
     
    # we place a bounding boxe around the faces
    print (&quot;number of faces&quot;, len(faces), &quot;image size&quot;, img.shape, &quot;image&quot;, image)
    for face in faces:
        cv2.rectangle(img, (face[0], face[1]), (face[0] + face[2], face[0] + face[3]), (255, 0, 0), 3)
         
    # we store the final result
    cv2.imwrite(image_out, img)
     
    # to see the image, press ESC to exit
    if show :
        cv2.imshow(&quot;face&quot;,img)
        if cv2.waitKey(5000) == 27: cv2.destroyWindow(&quot;face&quot;)
   
if __name__ == &quot;__main__&quot;:
    # wall lamp 
    for file in os.listdir(&quot;.&quot;&quot;) :
        if file.startswith(&quot;face&quot;) : continues # already processed
        if os.path.splitext(file)[-1].lower() in [&quot;.jpg&quot;,&quot;.jpeg&quot;,&quot;.png&quot; ] :
            face_detect (file, &quot;face_&quot; + file)
</code></pre></div><p>As you can see, it drew a blue square (bounding boxe) around the face in the image.</p> <h3 id="_8-3-feature-detection-and-description"><a href="#_8-3-feature-detection-and-description" class="header-anchor">#</a> 8.3.	Feature Detection and Description</h3> <p>In this section, we will present a brief description of the SIFT (Scale-Invariant Feature Transform) algorithm.
The main idea of this approach is to transform an image into feature vectors (feature maps), which should ideally be invariant to geometric transformations (rotation and scaling). This involves the detection of interest points, which will make it possible to detect an object. The detection of these points leads to the implementation of feature vectors whose components are specific to the point under consideration.</p> <p><em><strong>SIFT in OpenCV and Python:</strong></em></p> <div class="language- extra-class"><pre class="language-text"><code>import cv2
import numpy as np

img = cv2.imread('my_img.jpg')
gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

sift = cv2.SIFT()
kp = sift.detect(gray,None)

img=cv2.drawKeypoints(gray,kp)

cv2.imwrite('sift_img.jpg',img)

</code></pre></div><h3 id="_8-4-object-classification"><a href="#_8-4-object-classification" class="header-anchor">#</a> 8.4.	Object Classification</h3> <p>To correctly identify an object in an image, it may be interesting to simply detect its edges and shapes when extracting features.
How will we proceed to recognize objects?
These are the 3 steps that we will perform: (1) extracting features in the image, (2) estimating each feature and (3) classifying of edges.</p> <ul><li>Let's start by importing and loading an image.</li></ul> <div class="language- extra-class"><pre class="language-text"><code>import numpy as np 
import cv2 
image = cv2.imread('my_image.bmp')

</code></pre></div><ul><li>Step 1: Edge detection
In order to improve edge detection, we will convert the color image to grayscale before performing a thresholding.</li></ul> <div class="language- extra-class"><pre class="language-text"><code>gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
ret,thresh = cv2.threshold(gray,250,255,cv2.THRESH_BINARY_INV)

img,edges,h=cv2.findEdges(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)

</code></pre></div><ul><li>Step 2: Edge estimation</li></ul> <div class="language- extra-class"><pre class="language-text"><code>for cnt in edges:
perimeter =cv2.arcLength(cnt,True)
approx = cv2.approxPolyDP(cnt,0.01* perimeter,True)

M = cv2.moments(cnt)
cX = int(M[&quot;m10&quot;] / M[&quot;m00&quot;])
cY = int(M[&quot;m01&quot;] / M[&quot;m00&quot;])
cv2.drawEdges(image,[cnt],-1,(0,255,0),2)

</code></pre></div><ul><li>Step 3 : Pattern classification</li></ul> <div class="language- extra-class"><pre class="language-text"><code>All you need to do is to recall how many peaks there are in each shape.
</code></pre></div><div class="language- extra-class"><pre class="language-text"><code>if len(approx)==3:
shape = &quot;triangle&quot;
elif len(approx)==4:
(x, y, w, h) = cv2.boundingRect(approx)
ratio = w / float(h)
if ratio &gt;= 0.95 and ratio &lt;= 1.05:
shape = &quot; square&quot;
else:
shape = &quot;rectangle&quot;
elif len(approx)==5:
shape = &quot; pentagon&quot;
elif len(approx)==6:
shape = &quot; hexagon &quot;
else:
shape= &quot;circle&quot;
cv2.putText(image, shape, (cX, cY), cv2.FONT_HERSHEY_SIMPLEX,0.5, (255, 255, 255), 2)

</code></pre></div><p>We just have to display the result to check out our work:</p> <div class="language- extra-class"><pre class="language-text"><code>cv2.imshow('Final_image',image)
cv2.waitKey(0)
cv2.destroyAllWindows()
</code></pre></div><h2 id="_9-conclusions"><a href="#_9-conclusions" class="header-anchor">#</a> 9. Conclusions</h2> <p>In this guide, we discussed the topic of Computer Vision using OpenCV and Python. We presented some fundamental tasks of Computer Vision such as Object Recognition and Semantic Segmentation. We also examined some case studies about the process of edge and face detection, feature extraction and object classification.</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/Virgilio/inferno/time-series/time-series.html" class="prev">
        Time Series Master Guide
      </a></span> <span class="next"><a href="/Virgilio/inferno/computer-vision/object-instance-segmentation.html">
        Object Instance Segmentation using TensorFlow Framework and Cloud GPU Technology
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/Virgilio/assets/js/app.2e7e9464.js" defer></script><script src="/Virgilio/assets/js/2.f0423cde.js" defer></script><script src="/Virgilio/assets/js/15.971adb69.js" defer></script>
  </body>
</html>
