<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Object Tracking based on Deep Learning | Virgilio</title>
    <meta name="generator" content="VuePress 1.8.0">
    
    <meta name="description" content="Data Science E-Learning">
    
    <link rel="preload" href="/Virgilio/assets/css/0.styles.b60dbc94.css" as="style"><link rel="preload" href="/Virgilio/assets/js/app.3e5bc092.js" as="script"><link rel="preload" href="/Virgilio/assets/js/2.f0423cde.js" as="script"><link rel="preload" href="/Virgilio/assets/js/17.32bc2ea0.js" as="script"><link rel="prefetch" href="/Virgilio/assets/js/10.e5b0da50.js"><link rel="prefetch" href="/Virgilio/assets/js/11.e6289771.js"><link rel="prefetch" href="/Virgilio/assets/js/12.84859396.js"><link rel="prefetch" href="/Virgilio/assets/js/13.effd4111.js"><link rel="prefetch" href="/Virgilio/assets/js/14.afa7d231.js"><link rel="prefetch" href="/Virgilio/assets/js/15.971adb69.js"><link rel="prefetch" href="/Virgilio/assets/js/16.f313e168.js"><link rel="prefetch" href="/Virgilio/assets/js/18.d970d3c9.js"><link rel="prefetch" href="/Virgilio/assets/js/19.0dd10b9b.js"><link rel="prefetch" href="/Virgilio/assets/js/20.692fb2ad.js"><link rel="prefetch" href="/Virgilio/assets/js/21.c64c71fd.js"><link rel="prefetch" href="/Virgilio/assets/js/22.31363364.js"><link rel="prefetch" href="/Virgilio/assets/js/23.0118caf3.js"><link rel="prefetch" href="/Virgilio/assets/js/24.badcc79f.js"><link rel="prefetch" href="/Virgilio/assets/js/25.ea6ad12e.js"><link rel="prefetch" href="/Virgilio/assets/js/26.e543a3e8.js"><link rel="prefetch" href="/Virgilio/assets/js/27.87f2c03d.js"><link rel="prefetch" href="/Virgilio/assets/js/28.d13d6f4a.js"><link rel="prefetch" href="/Virgilio/assets/js/29.a0a64ca7.js"><link rel="prefetch" href="/Virgilio/assets/js/3.2723ef9f.js"><link rel="prefetch" href="/Virgilio/assets/js/30.2bd31805.js"><link rel="prefetch" href="/Virgilio/assets/js/31.197b8db6.js"><link rel="prefetch" href="/Virgilio/assets/js/32.b8aa0b5c.js"><link rel="prefetch" href="/Virgilio/assets/js/33.4dd225dc.js"><link rel="prefetch" href="/Virgilio/assets/js/34.1ba75d3d.js"><link rel="prefetch" href="/Virgilio/assets/js/35.fdf9a259.js"><link rel="prefetch" href="/Virgilio/assets/js/36.b6386cf5.js"><link rel="prefetch" href="/Virgilio/assets/js/37.35312921.js"><link rel="prefetch" href="/Virgilio/assets/js/38.ac063f29.js"><link rel="prefetch" href="/Virgilio/assets/js/39.d34154d2.js"><link rel="prefetch" href="/Virgilio/assets/js/4.03527a05.js"><link rel="prefetch" href="/Virgilio/assets/js/40.9b02d45f.js"><link rel="prefetch" href="/Virgilio/assets/js/41.201da11a.js"><link rel="prefetch" href="/Virgilio/assets/js/42.54c0ed8e.js"><link rel="prefetch" href="/Virgilio/assets/js/43.2a10bb86.js"><link rel="prefetch" href="/Virgilio/assets/js/44.7551723d.js"><link rel="prefetch" href="/Virgilio/assets/js/45.1da3fc09.js"><link rel="prefetch" href="/Virgilio/assets/js/46.39e01b18.js"><link rel="prefetch" href="/Virgilio/assets/js/47.215f2a77.js"><link rel="prefetch" href="/Virgilio/assets/js/48.89c344ee.js"><link rel="prefetch" href="/Virgilio/assets/js/49.5d1e05ec.js"><link rel="prefetch" href="/Virgilio/assets/js/5.2e206f8f.js"><link rel="prefetch" href="/Virgilio/assets/js/50.9e4f790d.js"><link rel="prefetch" href="/Virgilio/assets/js/6.3915b190.js"><link rel="prefetch" href="/Virgilio/assets/js/7.392b6a61.js"><link rel="prefetch" href="/Virgilio/assets/js/8.4f989c08.js"><link rel="prefetch" href="/Virgilio/assets/js/9.64d913f9.js">
    <link rel="stylesheet" href="/Virgilio/assets/css/0.styles.b60dbc94.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/Virgilio/" class="home-link router-link-active"><!----> <span class="site-name">Virgilio <span class="site-name2"> Data Science </span></span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://github.com/virgili0/Virgilio" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Contribute
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://github.com/virgili0/Virgilio" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Contribute
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/Virgilio/" aria-current="page" class="sidebar-link">What is Virgilio?</a></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Paradiso</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/paradiso/demystification-ai-ml-dl.html" class="sidebar-link">Demystification of the key concepts of AI and ML</a></li><li><a href="/Virgilio/paradiso/what-do-i-need-for-ml.html" class="sidebar-link">What do I need to do Machine Learning?</a></li><li><a href="/Virgilio/paradiso/do-you-really-need-ml.html" class="sidebar-link">Do you really need Machine Learning?</a></li><li><a href="/Virgilio/paradiso/use-cases.html" class="sidebar-link">Machine Learning use cases</a></li><li><a href="/Virgilio/paradiso/virgilio-teaching-strategy.html" class="sidebar-link">Virgilio's Teaching Strategy</a></li><li><a href="/Virgilio/paradiso/introduction-to-ml.html" class="sidebar-link">Introduction to Machine Learning</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Purgatorio</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading open"><span>Fundamentals</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/fundamentals/math-fundamentals.html" class="sidebar-link">Mathematics</a></li><li><a href="/Virgilio/purgatorio/fundamentals/statistics-fundamentals.html" class="sidebar-link">Statistics</a></li><li><a href="/Virgilio/purgatorio/fundamentals/python-fundamentals.html" class="sidebar-link">Python</a></li><li><a href="/Virgilio/purgatorio/fundamentals/jupyter-notebook.html" class="sidebar-link">Jupyter Notebook</a></li><li><a href="/Virgilio/purgatorio/fundamentals/the-data-science-process.html" class="sidebar-link">The Data Science Process</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Define The Scope and Ask Questions</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/define-the-scope-and-ask-questions/frame-the-problem.html" class="sidebar-link">Frame the Problem</a></li><li><a href="/Virgilio/purgatorio/define-the-scope-and-ask-questions/usage-and-integration.html" class="sidebar-link">Usage and Integration</a></li><li><a href="/Virgilio/purgatorio/define-the-scope-and-ask-questions/starting-a-data-project.html" class="sidebar-link">Starting a Data Project</a></li><li><a href="/Virgilio/purgatorio/define-the-scope-and-ask-questions/workspace-setup-and-cloud-computing.html" class="sidebar-link">Workspace Setup and Cloud Computing</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Collect and Prepare Data</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/collect-and-prepare-data/data-collection.html" class="sidebar-link">Data Collection</a></li><li><a href="/Virgilio/purgatorio/collect-and-prepare-data/data-preparation.html" class="sidebar-link">Data Preparation</a></li><li><a href="/Virgilio/purgatorio/collect-and-prepare-data/data-visualization.html" class="sidebar-link">Data Visualization</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Select and Train Machine Learning Models</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/select-and-train-machine-learning-models/machine-learning-theory.html" class="sidebar-link">Machine Learning Theory</a></li><li><a href="/Virgilio/purgatorio/select-and-train-machine-learning-models/deep-learning-theory.html" class="sidebar-link">Deep Learning Theory</a></li><li><a href="/Virgilio/purgatorio/select-and-train-machine-learning-models/evaluation-and-finetuning.html" class="sidebar-link">Evaluation and Fine Tuning</a></li><li><a href="/Virgilio/purgatorio/select-and-train-machine-learning-models/tools-and-libraries.html" class="sidebar-link">Tools and Libraries</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Launch and Mantain the System </span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/launch-and-mantain-the-system/serving-trained-models.html" class="sidebar-link">Serving Trained Models</a></li><li><a href="/Virgilio/purgatorio/launch-and-mantain-the-system/monitoring-usage-and-behavior.html" class="sidebar-link">Monitoring Usage and Behavior</a></li><li><a href="/Virgilio/purgatorio/launch-and-mantain-the-system/automation-and-reproducibility.html" class="sidebar-link">Automation and Reproducibility</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Now Go Build </span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/now-go-build/a-messy-real-world.html" class="sidebar-link">A Messy Real World</a></li><li><a href="/Virgilio/purgatorio/now-go-build/transfer-learning.html" class="sidebar-link">Transfer Learning</a></li><li><a href="/Virgilio/purgatorio/now-go-build/best-practices.html" class="sidebar-link">Best Practices</a></li></ul></section></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Inferno</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Welcome to Inferno</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/inferno/welcome-to-inferno/welcome-to-inferno.html" class="sidebar-link">Welcome to Inferno</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Time Series</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/inferno/time-series/introduction-to-time-series.html" class="sidebar-link">Introduction to Time Series</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading open"><span>Computer Vision</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/inferno/computer-vision/introduction-to-computer-vision.html" class="sidebar-link">Introduction to Computer Vision using OpenCV and Python</a></li><li><a href="/Virgilio/inferno/computer-vision/object-instance-segmentation.html" class="sidebar-link">Object Instance Segmentation using TensorFlow Framework and Cloud GPU Technology</a></li><li><a href="/Virgilio/inferno/computer-vision/object-tracking.html" aria-current="page" class="active sidebar-link">Object Tracking based on Deep Learning</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/Virgilio/inferno/computer-vision/object-tracking.html#what-is-video-tracking" class="sidebar-link">What is Video tracking?</a></li><li class="sidebar-sub-header"><a href="/Virgilio/inferno/computer-vision/object-tracking.html#conventional-methods-for-object-detection-and-tracking" class="sidebar-link">Conventional methods for object detection and tracking</a></li><li class="sidebar-sub-header"><a href="/Virgilio/inferno/computer-vision/object-tracking.html#deep-learning-based-methods-for-object-detection-and-tracking" class="sidebar-link">Deep Learning based methods for object detection and tracking</a></li><li class="sidebar-sub-header"><a href="/Virgilio/inferno/computer-vision/object-tracking.html#use-case-motion-tracker" class="sidebar-link">Use case: Motion Tracker</a></li></ul></li><li><a href="/Virgilio/inferno/computer-vision/Object_detection_based_on_Deep_Learning.html" class="sidebar-link">Object detection based on Deep Learning</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Soft Skills</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/inferno/soft-skills/impactful-presentations.html" class="sidebar-link">Impactful Presentations</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Tools</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/inferno/tools/geo-gebra.html" class="sidebar-link">Geo Gebra</a></li><li><a href="/Virgilio/inferno/tools/latex.html" class="sidebar-link">LaTex</a></li><li><a href="/Virgilio/inferno/tools/regex.html" class="sidebar-link">Regex introduction</a></li><li><a href="/Virgilio/inferno/tools/wolfram-alpha.html" class="sidebar-link">Wolfram Alpha</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Research</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/inferno/research/zotero.html" class="sidebar-link">Zotero</a></li><li><a href="/Virgilio/inferno/research/sota-papers.html" class="sidebar-link">Research papers explained</a></li></ul></section></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="object-tracking-based-on-deep-learning"><a href="#object-tracking-based-on-deep-learning" class="header-anchor">#</a> <strong>Object Tracking based on Deep Learning</strong></h1> <p><img src="https://i.ibb.co/0JXbPjm/Capture.png" alt="Fig1"></p> <h2 id="what-is-video-tracking"><a href="#what-is-video-tracking" class="header-anchor">#</a> What is Video tracking?</h2> <p>Target tracking is the process of locating moving targets in a video camera for a very wide range of real-world applications. Real-time target tracking is an important task for many computer vision applications, such as surveillance, perception-based user interfaces, augmented reality, object-based video compression and autonomous driving.</p> <p><img src="https://i.ibb.co/KymdPrW/2.png" alt="Fig2"></p> <p>Historically, there are many ways to track video targets: when you track all moving objects, the difference between the images becomes useful; for tracking the moving hand in the video, the average shift method based on skin color is the best solution; Model matching is a good technique for tracking an aspect of an object.</p> <p>Since the results of the <a href="http://www.image-net.org/challenges/LSVRC/2012/" target="_blank" rel="noopener noreferrer">ImageNet 2012 challenge<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, Deep Learning (and in particular, Convolutional Neural Networks (CNNs)) has become the main method for solving this kind of problem. Object tracking studies have therefore naturally integrated recognition models, which has made it possible to create tracking algorithms.</p> <p>For more details, you can look at the following projects and tutorials related to the video track challenge:</p> <ul><li><p><a href="https://cv-tricks.com/object-tracking/quick-guide-mdnet-goturn-rolo/" target="_blank" rel="noopener noreferrer">Zero to Hero: A Quick Guide to Object Tracking: MDNET, GOTURN, ROLO
<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p><a href="https://www.move-lab.com/blog/tracking-things-in-object-detection-videos" target="_blank" rel="noopener noreferrer">TRACKING THINGS IN OBJECT DETECTION VIDEOS
<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p><a href="https://medium.com/@manivannan_data/multiple-object-tracking-algorithms-a01973272e52" target="_blank" rel="noopener noreferrer">Multiple Object Tracking Algorithms
<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p><a href="https://github.com/OlafenwaMoses/ImageAI/blob/master/imageai/Detection/VIDEO.md" target="_blank" rel="noopener noreferrer">ImageAI : Video Object Detection, Tracking and Analysis<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p><a href="https://github.com/DrewNF/Tensorflow_Object_Tracking_Video" target="_blank" rel="noopener noreferrer">Tensorflow Object Tracking Video<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li></ul> <p>Practical books that will allow you to learn the different aspects of video tracking:</p> <ol><li><a href="https://www.amazon.com/Video-Tracking-Practice-Emilio-Maggio/dp/0470749644" target="_blank" rel="noopener noreferrer">Video Tracking: Theory and Practice 1st Edition<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://www.amazon.com/Video-object-Tracking-Image-Processing/dp/3844386238" target="_blank" rel="noopener noreferrer">Video object Tracking: Image Processing and Tracking Paperback – July 16, 2011<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ol> <h2 id="conventional-methods-for-object-detection-and-tracking"><a href="#conventional-methods-for-object-detection-and-tracking" class="header-anchor">#</a> Conventional methods for object detection and tracking</h2> <p><strong>1. Basic object detection</strong></p> <p>To make a baseline movement detection, given the difference between the &quot;background&quot; and the other frames, this method is still quite good, but you must first define the background frame, if it is outside, changes in lighting can cause a false detection. Therefore, this method is very limited.
OpenCV offers a class, called BackgroundSubtractor, which is useful for splitting the foreground from the background. There are three background separators in OpenCV3: <em>K-Nearest (KNN)</em>,* Gaussian Mixture (MOG2)*, and <em>Geometric Multigid (GMG)</em>. The BackgroundSubtractor class used for video analysis, i.e. the BackgroundSubtractor class <em>learns</em> the context of each image. The BackgroundSubtractor class is often used to compare different frames as well as to record previous frames, which can be used to improve the results of motion analysis.
<img src="https://www.researchgate.net/profile/Venkatesh_Saligrama/publication/224396654/figure/download/fig1/AS:393809457369095@1470902899537/Background-subtraction-results-for-synthetic-objects-moving-against-quasi-static.png" alt="Img3"></p> <p><strong>2. Background splitter by MOG2</strong></p> <p>One of the basic features of the BackgroundSubtractor class is that it can compute shadows. This is absolutely essential for accurate playback of video images: by detecting shadows, it is possible to exclude shadow areas from the detected image (in threshold mode) so that real attributes can be focused.
<img src="data:image/png;base64,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" alt="image.png"></p> <p><strong>3. Background splitter by KNN</strong></p> <p>The images can be viewed from left to right: detected moving targets, background segmentation, thresholding after background segmentation.
<img src="https://www.ccoderun.ca/programming/doxygen/opencv/Background_Subtraction_Tutorial_Scheme.png" alt="Fig5"></p> <p><strong>4. Kalman object tracking</strong></p> <p>Kalman is a Hungarian mathematician, who developed a filter from his PhD thesis work and the 1960 <a href="https://asmedigitalcollection.asme.org/fluidsengineering/article/82/1/35/397706/A-New-Approach-to-Linear-Filtering-and-Prediction" target="_blank" rel="noopener noreferrer">paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> entitled &quot;A New Approach to Linear Filtering and Prediction Problems&quot;.</p> <p>Kalman filtering has been applied in many domains, particularly in the navigation guidance of aircraft and missiles.
The Kalman filter works repeatedly on a noisy input data stream (such as a video input in computer vision) and produces a statistically optimal estimate of the state of the underlying system (such as the position in the video).</p> <p>The Kalman filter algorithm is divided into two phases:</p> <ul><li>Prediction phase: the Kalman filter uses the covariance computed from the current position to estimate the target's new position.</li> <li>Update phase: the Kalman filter stores the target position and calculates the corrected covariance for the next iteration.</li></ul> <p><img src="https://i.stack.imgur.com/zS2OB.png" alt="Fig5"></p> <h2 id="deep-learning-based-methods-for-object-detection-and-tracking"><a href="#deep-learning-based-methods-for-object-detection-and-tracking" class="header-anchor">#</a> Deep Learning based methods for object detection and tracking</h2> <p>In recent years, Deep Learning methods have been successfully applied in the field of object tracking and are gradually exceeding traditional performance methods. In this section, we will present current target tracking algorithms based on Deep Learning.</p> <p><strong>1. Trends in object tracking</strong></p> <p>Unlike the trend towards of Deep Learning in the visual domain, such as detection and recognition, the application of this paradigm in the object tracking domain is not seamless. The main problem is the lack of learning data: one of the complications of the deep model comes from the effective learning of a large number of labelled learning data, while target tracking only provides the context for selecting the first image as learning data. In this case, it is difficult to train a deep model from scratch at the beginning of the tracking. Currently, the target tracking algorithm based on Deep Learning takes several ideas to solve this problem, including the following and finally the recurrent neural network in the current tracking field to solve the target tracking problem. Where training data for target tracking is very limited, complementary non-supervised training data is used for pre-training to achieve a high-level feature epresentation of object, and in actual tracking, using the limited sample information from the current tracking target. The accuracy of the pre-training model confers a higher classification performance on the model for the current tracking goal, which significantly reduces the need to track target training samples and improves the performance of the tracking algorithm.</p> <p><strong>2. Overall multi-target tracking process</strong></p> <p>In order to track a target, first this target is detected. This step is called <em>target detection</em>, then the target in each image is mapped based on the detection result. Today, there are multi-target detectors, such as <a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener noreferrer">SSD<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> and <a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener noreferrer">YOLO<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, etc.</p> <p><strong>3. State-of-the-art methods for object tracking</strong></p> <h5 id="_3-1-goturn"><a href="#_3-1-goturn" class="header-anchor">#</a> 3.1.GOTURN</h5> <p>A further great strength of deep learning is the end-to-end learning process. We believe that this opens up a promising future for tracking. Here is an example of the <a href="https://www.learnopencv.com/goturn-deep-learning-based-object-tracking/" target="_blank" rel="noopener noreferrer"><em>GOTURN</em> method<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>. GOTURN's current method has been included in OpenCV 3.2.0 development version.</p> <p><img src="https://www.learnopencv.com/wp-content/uploads/2018/07/goturn-inputs-ouputs-1024x487.jpg" alt="Fig6">
GOTURN involves a convolution network based on the input of a pair of images using the ALOV300+ video sequence set and the ImageNet sensing data set, and generates the position change from the previous frame in the detection area to obtain the target's position on the current frame.</p> <h5 id="_3-2-specific-target-tracking"><a href="#_3-2-specific-target-tracking" class="header-anchor">#</a> 3.2. Specific-target tracking</h5> <p>In practice, a significant aspect of tracking is the tracking of specific objects, such as face tracking, gesture tracking and human tracking. Tracking a particular object is different from the approach described above and relies more on the training of a particular detector. Due to its obvious features, face tracking is mainly implemented through detection task, such as the state-of-the-art Viola-Jones detection model and the current face detection or face point detection model using Deep learning.</p> <h5 id="_3-3-compression-tracking"><a href="#_3-3-compression-tracking" class="header-anchor">#</a> 3.3. Compression tracking</h5> <p>This approach involves using the compressed detection method to represent feature maps, by achieving dimensional reduction and obtaining small-size cues in order to capture large-size feature space (The block diagram is shown in Figure).
<img src="https://img-blog.csdn.net/20170418162336895?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2hmc2h1YWlzaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="Fig7"></p> <h2 id="use-case-motion-tracker"><a href="#use-case-motion-tracker" class="header-anchor">#</a> Use case: Motion Tracker</h2> <p>Here, we will see how to track the motion of moving objects in the video using OpenCV 3.0 and basic techniques (MOG2). Motion tracking is used to track the motion of objects and then transmit the detected information to an application for further processing.</p> <p><img src="https://i.ytimg.com/vi/XmI2kE2hUgE/maxresdefault.jpg" alt="Fig7"> <strong>Basic dependencies:</strong></p> <ul><li>OpenCV 3.0;</li> <li>Numpy lib.</li></ul> <p>The tracker implementation is as follows:</p> <div class="language- extra-class"><pre class="language-text"><code>%%writefile points.py
import numpy as np
import numpy.linalg.linalg as la
import matplotlib.pyplot as plt

fst = lambda x: x[0]
snd = lambda x: x[1]

distance = lambda p1,p2: la.norm(p1 - p2)

matchPaths = lambda r, a, paths: [neighborhoodPath(r,paths,ai) for ai in a]

neighborhoodPath = lambda r, paths, pnt: [path for path in paths if distance(path[0],pnt) &lt; r]

def itemMatcher(choice, items):
    items = list(sorted(items, key = lambda x: len(x[1])))
    accumulator = []
    for index in range(len(items)):
        (a, bs) = items[index]
        if bs == []:
            accumulator.append((a, None))
        else:
            b = choice(a,bs)
            accumulator.append((a, b))
            for ind in range(len(items)):
                (a2, bs2) = items[ind]
                if any(np.array_equal(b,belement) for belement in bs):
                    bs2 = [belement for belement in bs2 if not np.array_equal(belement, b)]
                items[ind] = (a2, bs2)
    return accumulator


def extendPaths(r, paths, scatter, filterWith, noisy = False, discard = True):
    matches = matchPaths(r,scatter,paths)
    zipped = zip(scatter, matches)
    def choice(point, pathOptions): 
        return max(pathOptions,key=len)
    def combine(tup):
        (pnt, val) = tup
        if val == None:
            if not noisy:
                return [pnt]
            else:
                return []
        else:
            return [pnt] + val
    lst = itemMatcher(choice, zipped)
    extended_paths = map(snd,lst)
    if discard:
       return filter(lambda x: x != [], map(combine,lst))
    unextended_paths = [p for p in paths if not array_in(p,extended_paths)]  
    unextended_paths = filter(filterWith, unextended_paths)
    return ( unextended_paths, filter(lambda x: x!=[], map(combine, lst)) ) # First element is paths to be archived, second element is the extended paths
   

def array_in(arr, lst):
    return any(np.array_equal(arr,elem) for elem in lst)

import pickle
def loaddata(filename):
    return pickle.load(open(filename))

def stringPaths(r, scatters):
    paths = []
    for sc in scatters:
        paths = extendPaths(r, paths, sc)
    return paths

def plotit(paths):
    p = [reduce(lambda x,y: np.append(x,y,axis=0),pa) for pa in paths]
    p = map(lambda x: x.T, p)
    plt.hold(True)
    map(lambda x: plt.plot(x[0],x[1],'x'),p)

def shortcut(r, filename):
    plotit(stringPaths(r, loaddata(filename)))
def rawpoints(filename):
    scatters = loaddata(filename)
    plotit(scatters)
</code></pre></div><div class="language- extra-class"><pre class="language-text"><code>from __future__ import division
import numpy as np
import cv2
import scipy
import pickle
import points

from sys import argv

OBJ = True
### THE SETTING PARAMETERS FOR VIDEO.MP4
if OBJ:
    KERN_SIZE = 8
    RADIUS = 20
    THRESHOLD_AT = 100
    INPUT_SIZE_THRESHOLD = 75
    MIN_PATH_SIZE = 10
    MIN_PATH_STD = 3*RADIUS
    WRITE_TO_FILE = True
###END PARAMETERS
else:
    KERN_SIZE = 8
    RADIUS = 5
    THRESHOLD_AT = 127
    INPUT_SIZE_THRESHOLD = 150
    MIN_PATH_SIZE = 10
    MINI_PATH_STD = 3*RADIUS
    WRITE_TO_FILE = True

video_output = &quot;output_tracking.avi&quot;

def avgit(y):
    return x.sum(axis=0)/np.shape(y)[0]
def plotp(p,mat,color=0):
    mat[p[0,1],p[0,0]] = color

if len(argv) != 3:
    cap = cv2.VideoCapture('vid1.mp4')
else:
    cap = cv2.VideoCapture(argv[1])
    video_outputfile = argv[2]
fourcc = cv2.VideoWriter_fourcc(*'XVID')
ret, frame = cap.read()
height, width, layers = frame.shape
video_out = cv2.VideoWriter(video_outputfile, fourcc, 30, (width, height), True)
print video_out.isOpened()

fgbg = cv2.createBackgroundSubtractorMOG2()
bwsub= cv2.createBackgroundSubtractorMOG2()

kernlen = KERN_SIZE
kern = np.ones((kernlen,kernlen))/(kernlen**2)
ddepth = -1
def blur(image):
    return cv2.filter2D(image,ddepth,kern)
def blr_thr(image, val=127):
    return cv2.threshold(blur(image),val,255,cv2.THRESH_BINARY)[1]
def normalize(image):
    s = np.sum(image)
    if s == 0:
       return image
    return height*width* image / s
#collection = []
paths = []
archive = []

r = RADIUS
thresh_at = THRESHOLD_AT
THIS_MUCH_IS_NOISE = INPUT_SIZE_THRESHOLD

while(cap.isOpened()):
    ret, frame = cap.read()
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    fgmask = fgbg.apply(frame)
    mask = blur(fgmask)
    ret2, mask = cv2.threshold(mask, thresh_at, 255, cv2.THRESH_BINARY)
    
    res = cv2.findContours(mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
    cons = res[1]
    scatter = map(avgit, cons)
    filterWith = lambda x: len(x) &gt; MIN_PATH_SIZE and np.std(x) &gt; MINIMUM_PATH_STD
    (toArchive, paths) = points.extendPaths(r, paths, scatter, filterWith, noisy=(len(scatter) &gt; THIS_MUCH_IS_NOISE), discard=False)
    archive += toArchive
    img = (1 - mask)*gray
    for path in archive:
        #color = 255
        cv2.polylines(img, np.int32([reduce(lambda x,y: np.append(x,y,axis=0), path)]), 0, (255,0,0))
    for path in paths:
        cv2.polylines(img, np.int32([reduce(lambda x,y: np.append(x,y,axis=0), path)]), 1, (0,0,255))
    cv2.imshow('frame', img)
    video_out.write(img)
    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break
cap.release()
video_out.release()
cv2.destroyAllWindows()


if WRITE_TO_FILE:
    pickle.dump(archive, open('my_path' + str(np.floor(1000*np.random.rand())) + '.pickle','w'))
</code></pre></div></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/Virgilio/inferno/computer-vision/object-instance-segmentation.html" class="prev">
        Object Instance Segmentation using TensorFlow Framework and Cloud GPU Technology
      </a></span> <span class="next"><a href="/Virgilio/inferno/computer-vision/Object_detection_based_on_Deep_Learning.html">
        Object detection based on Deep Learning
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/Virgilio/assets/js/app.3e5bc092.js" defer></script><script src="/Virgilio/assets/js/2.f0423cde.js" defer></script><script src="/Virgilio/assets/js/17.32bc2ea0.js" defer></script>
  </body>
</html>
